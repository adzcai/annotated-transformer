{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f82f4e-42a9-4918-b8ce-f591829852a3",
   "metadata": {},
   "source": [
    "## 1. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4857f8-3269-4840-927b-b9a77ffd148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uq matplotlib spacy torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec4987-fe2b-4734-b892-addddb4638f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aae8ef-7814-4d2b-bbd4-c5e004324a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from architecture.utils import subsequent_mask, PositionalEncoding\n",
    "from architecture.transformer import make_model, EncoderDecoder\n",
    "from optimize import LabelSmoothing, SimpleLoss, get_lr, get_scheduler, DummyOptimizer, DummyScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844cd318-a485-4c8f-ad4d-926a6a6222ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token masking\n",
    "plt.imshow(subsequent_mask(20)[0])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03daf111-767e-40a8-a301-ed4a7a218e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding\n",
    "pe = PositionalEncoding(d_model=20)\n",
    "y = pe(torch.zeros(1, 100, 20))\n",
    "\n",
    "plt.plot(torch.arange(100), y[0, :, 4:8])\n",
    "plt.legend([f\"dim {p}\" for p in range(4, 8)])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8310c-2b1d-419f-b71d-7520267616c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noam learning rate scheduler\n",
    "\n",
    "h_params = ((512, 4000), (512, 8000), (256, 4000))\n",
    "opts = [\n",
    "    [get_lr(step, d_model=d_model, scale=1., n_warmup_steps=steps) for d_model, steps in h_params]\n",
    "    for step in range(1, 20000)\n",
    "]\n",
    "\n",
    "plt.plot(torch.arange(1, 20000), opts)\n",
    "plt.legend([f\"{d_model}:{steps}\" for d_model, steps in h_params])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef2385-35ab-4b5d-8c9b-0a5ccbebebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same graph as above but actually running the scheduler on a dummy model\n",
    "\n",
    "dummy_model = nn.Linear(1, 1)\n",
    "lr = []\n",
    "\n",
    "for i, (d_model, n_warmup_steps) in enumerate(h_params):\n",
    "    optimizer, lr_scheduler = get_scheduler(dummy_model, d_model=d_model, n_warmup_steps=n_warmup_steps, lr=1.)\n",
    "    tmp = []\n",
    "    for step in range(20000):\n",
    "        tmp.append(optimizer.param_groups[0]['lr'])\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "    lr.append(tmp)\n",
    "\n",
    "plt.plot(torch.arange(0, 20000), list(zip(*lr)))\n",
    "plt.legend([f\"{d_model}:{steps}\" for d_model, steps in h_params])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978c257-dd1a-42c4-80d5-77c44d566d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label smoothing\n",
    "crit = LabelSmoothing(n_classes=5, padding_idx=0, smoothing=0.4)\n",
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0] for _ in range(5)])\n",
    "v = crit(predict.log(), torch.LongTensor([2, 1, 0, 3, 3]))\n",
    "\n",
    "plt.imshow(crit.true_dist)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c327e98-2df3-4ba7-849c-423bf81edde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, smoothing=0.1):\n",
    "    crit = LabelSmoothing(5, 0, smoothing)\n",
    "    \n",
    "    d = x + 3\n",
    "    predict = torch.FloatTensor([[0, x/d, 1/d, 1/d, 1/d]])\n",
    "    \n",
    "    return crit(predict.log(), torch.LongTensor([1])).item()\n",
    "\n",
    "smoothing = (0.1, 0.3, 0.5)\n",
    "plt.plot(torch.arange(1, 100), [[loss(x, s) for s in smoothing] for x in range(1, 100)])\n",
    "plt.legend(smoothing)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Confidence\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727d55d-5940-4e35-a183-518343c57b65",
   "metadata": {},
   "source": [
    "## 2. Training the copy task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b72a3-1302-4564-9544-17408f7cbc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architecture.transformer import ModelConfig\n",
    "from train import run_epoch\n",
    "from preprocess import Batch\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9eb2b-9a61-4577-ac47-3a8044199244",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = 11\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a378bce1-1657-49a9-ba2b-db3b4a69edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(n_vocab: int, batch_size: int, n_batches: int):\n",
    "    for i in range(n_batches):\n",
    "        data = torch.randint(1, n_vocab, size=(batch_size, 10)).detach()\n",
    "        data[:, 0] = 1\n",
    "        yield Batch(src=data.clone(), tgt=data.clone(), pad_token=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf8aa0-a159-4d9b-b076-a406772fd290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_copy(model: nn.Module):\n",
    "    criterion = LabelSmoothing(n_classes=n_vocab, padding_idx=0, smoothing=0.)\n",
    "\n",
    "    loss_fn = SimpleLoss(model.generator, criterion)\n",
    "    optimizer, scheduler = get_scheduler(model)\n",
    "\n",
    "    for epoch in trange(1, 11, desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        fake_data = data_gen(n_vocab, batch_size=30, n_batches=20)\n",
    "        run_epoch(\n",
    "            fake_data,\n",
    "            model,\n",
    "            loss_compute=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            mode=\"train\"\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            fake_valid = data_gen(n_vocab, batch_size=30, n_batches=5)\n",
    "            run_epoch(\n",
    "                fake_valid,\n",
    "                model,\n",
    "                loss_fn,\n",
    "                DummyOptimizer(),\n",
    "                DummyScheduler(),\n",
    "                mode=\"eval\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac1bb28-7bd0-4b5a-b642-9920c3943ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = ModelConfig(n_vocab, n_vocab, n_layers=2)\n",
    "model = make_model(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d014cc1-feef-4ee1-a3b3-2cb499b7e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64203d53-18a1-44e3-af13-e1c34e1b56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/copy_10_epochs.pt'\n",
    "\n",
    "if True:\n",
    "    torprinth.save(model.state_dict(), model_path)\n",
    "    print(f'saved model to {model_path}')\n",
    "\n",
    "if False:\n",
    "    state_dict = torch.load(model_path)\n",
    "    model = make_model(model_cfg)\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f'loaded model from {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a83b9-4be8-47da-a71c-2e89632e2ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate it on the copy task\n",
    "\n",
    "seq_len = 10\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        src = torch.empty(1, seq_len, dtype=torch.long)\n",
    "        src[0, 0] = 1\n",
    "        src[0, 1:] = torch.randint(2, n_vocab, size=(1, seq_len - 1))\n",
    "        src_mask = torch.ones(1, 1, seq_len, dtype=torch.bool)\n",
    "        output = model.greedy_decode(src, src_mask, n_ctx=seq_len, start_token=1)\n",
    "        print(torch.cat((src, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f401b-c32d-4496-8b94-928085ac38ec",
   "metadata": {},
   "source": [
    "## To the real world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc9768-4dcc-4082-bc71-dc7b227504b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python -m spacy download en_core_web_sm > /dev/null\n",
    "! python -m spacy download de_core_news_sm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14485500-abc5-4760-9c94-0cce2d863ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uq GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd33f2-0212-4357-84cb-4e8ea97a2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model, TrainingConfig\n",
    "from preprocess import build_vocab\n",
    "import spacy, os\n",
    "\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da508029-b3d2-409e-9b49-39e5932a76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_src, vocab_tgt = build_vocab(spacy_de, spacy_en)\n",
    "len(vocab_src), len(vocab_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1794e-9c08-49c2-b545-d53b0cd5151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = ModelConfig(n_src_vocab=len(vocab_src), n_tgt_vocab=len(vocab_tgt))\n",
    "\n",
    "train_cfg = TrainingConfig(\n",
    "    batch_size = 32,\n",
    "    distributed = True,\n",
    "    n_epochs = 8,\n",
    "    accum_interval = 10,\n",
    "    lr_init = 1.,\n",
    "    n_ctx = 72,\n",
    "    n_warmup_steps = 3000,\n",
    "    file_prefix = \"models/multi30k_epoch_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba751596-177f-47bf-99bb-cb14f94a1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, model_cfg, train_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f00fff-fed7-4517-97a4-20e92c7dd276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
