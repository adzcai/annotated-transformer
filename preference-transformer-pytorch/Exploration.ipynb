{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement latexify-py (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for latexify-py\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -Uq \"latexify-py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import latexify\n",
    "\n",
    "def get_fixed_positional_embeddings(d_model: int, n_ctx: int):\n",
    "    \"\"\"\n",
    "    Get fixed positional embeddings for the input.\n",
    "    \"\"\"\n",
    "\n",
    "    assert d_model % 2 == 0\n",
    "\n",
    "    position = torch.arange(n_ctx)[None, :]\n",
    "\n",
    "    div_term = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uq \"gym[box2d]\" moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.9/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/alexandercai/Developer/ml/preference-transformer-pytorch/recordings folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = RecordVideo(gym.make('LunarLander-v2', render_mode=\"human\"), './recordings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:59: UserWarning: \u001b[33mWARN: Disabling video recorder because environment <TimeLimit<OrderEnforcing<PassiveEnvChecker<LunarLander<LunarLander-v2>>>>> was not initialized with any compatible video mode between `rgb_array` and `rgb_array_list`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminated, truncated = False, False\n",
    "while not terminated and not truncated:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madd_bias_kv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madd_zero_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Allows the model to jointly attend to information\n",
      "from different representation subspaces as described in the paper:\n",
      "`Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\n",
      "\n",
      "Multi-Head Attention is defined as:\n",
      "\n",
      ".. math::\n",
      "    \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
      "\n",
      "where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
      "\n",
      "``forward()`` will use a special optimized implementation if all of the following\n",
      "conditions are met:\n",
      "\n",
      "- self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor. This\n",
      "  restriction will be loosened in the future.)\n",
      "- Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n",
      "- training is disabled (using ``.eval()``)\n",
      "- dropout is 0\n",
      "- ``add_bias_kv`` is ``False``\n",
      "- ``add_zero_attn`` is ``False``\n",
      "- ``batch_first`` is ``True`` and the input is batched\n",
      "- ``kdim`` and ``vdim`` are equal to ``embed_dim``\n",
      "- at most one of ``key_padding_mask`` or ``attn_mask`` is passed\n",
      "- if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n",
      "  nor ``attn_mask`` is passed\n",
      "\n",
      "If the optimized implementation is in use, a\n",
      "`NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n",
      "``query``/``key``/``value`` to represent padding more efficiently than using a\n",
      "padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n",
      "will be returned, and an additional speedup proportional to the fraction of the input\n",
      "that is padding can be expected.\n",
      "\n",
      "Args:\n",
      "    embed_dim: Total dimension of the model.\n",
      "    num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
      "        across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
      "    dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n",
      "    bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n",
      "    add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n",
      "    add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n",
      "        Default: ``False``.\n",
      "    kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n",
      "    vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n",
      "    batch_first: If ``True``, then the input and output tensors are provided\n",
      "        as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "    >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           /usr/local/anaconda3/lib/python3.9/site-packages/torch/nn/modules/activation.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     MultiheadAttention\n"
     ]
    }
   ],
   "source": [
    "?MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
